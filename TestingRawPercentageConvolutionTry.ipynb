{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/plantehenry/NeuralNetworksFinalProject/blob/main/TestingRawPercentageConvolutionTry.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.datasets import CIFAR100, CIFAR10\n",
        "from torch.utils.data import DataLoader, Dataset, random_split\n",
        "from torchvision.transforms.functional import resize\n",
        "from torchvision.transforms import CenterCrop\n",
        "from torchvision.transforms import ToTensor\n",
        "from torchvision.io import read_image\n",
        "from torchsummary import summary\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "nfCs7DH7ex1L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "PgNzdFYEpg_S",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9ecf5b07-6bf0-475e-e45f-a894231af5ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define Network"
      ],
      "metadata": {
        "id": "pSjILg0Se0_M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CNN(nn.Module):\n",
        "\n",
        "    def __init__(self, numChannels):\n",
        "        super(CNN, self).__init__()\n",
        "\n",
        "        # Convolutional layers:\n",
        "        self.conv1 = nn.Conv2d(in_channels = numChannels, out_channels = 7, kernel_size = (1, 10), stride = 1)\n",
        "        self.conv2 = nn.Conv2d(in_channels = 7, out_channels = 15, kernel_size = (1, 5), stride = (1,1))\n",
        "        self.conv3 = nn.Conv2d(in_channels = 15, out_channels = 15, kernel_size = (1, 3), stride = (1,1))\n",
        "        # self.conv4 = nn.Conv2d(in_channels = 384, out_channels = 384, kernel_size = (1, 3), stride = (1,1))\n",
        "        # self.conv5 = nn.Conv2d(in_channels = 384, out_channels = 256, kernel_size = (1, 3), stride = (1,1))\n",
        "\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size = (1,2), stride = (1,2))\n",
        "\n",
        "        # Batch normalization layers:\n",
        "        self.batchnorm1 = nn.BatchNorm2d(num_features = 30)\n",
        "        self.batchnorm2 = nn.BatchNorm2d(num_features = 50)\n",
        "\n",
        "        # Fully-connected layers:\n",
        "        self.fc1 = nn.Linear(in_features = 60, out_features= 3)\n",
        "        self.fc2 = nn.Linear(in_features = 512, out_features= 4)\n",
        "\n",
        "        self.dropout1 = nn.Dropout2d(p = args.dropoutRate)\n",
        "        self.dropout2 = nn.Dropout2d(p = args.dropoutRate)\n",
        "        self.dropout3 = nn.Dropout2d(p = args.dropoutRate)\n",
        "        self.dropout4 = nn.Dropout(p = args.dropoutRate)\n",
        "\n",
        "    # Evaluation function\n",
        "    def evaluate(self, model, dataloader, device):\n",
        "        model.eval()\n",
        "        running_loss = 0.0\n",
        "        criterion = nn.MSELoss()  \n",
        "        for data in dataloader:\n",
        "                inputs, true = data\n",
        "                inputs = inputs.to(device)\n",
        "                true = true.to(device)\n",
        "                outputs = model(inputs)\n",
        "\n",
        "                loss = criterion(outputs, true)\n",
        "\n",
        "                loss = loss.detach().cpu().numpy()\n",
        "                running_loss += loss\n",
        "        return(running_loss/ dataloader.__len__())\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.dropout1(x)\n",
        "        x = self.maxpool(x)\n",
        "        # x = self.batchnorm1(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.dropout2(x)\n",
        "        x = self.maxpool(x)\n",
        "        # x = self.batchnorm2(x)\n",
        "        x = self.conv3(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.dropout3(x)\n",
        "        # x = self.conv4(x)\n",
        "        # x = self.relu(x)\n",
        "        # x = self.conv5(x)\n",
        "        # x = self.relu(x)\n",
        "        x = self.maxpool(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.fc1(x)\n",
        "        # x = self.relu(x)\n",
        "        # x = self.fc2(x))\n",
        "\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "bU3lPzXCXOpj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Loaders"
      ],
      "metadata": {
        "id": "RyhmhjrG4Qt9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import argparse\n",
        "from torch.utils.data.dataloader import default_collate\n",
        "import csv\n",
        "\n",
        "\n",
        "class TrainData(torch.utils.data.Dataset):\n",
        "    def __init__(self,args):\n",
        "        self.args = args\n",
        "        self.input_sequence , self.output_sequence = self.loadData()\n",
        "    ## working on giving more than one sequence\n",
        "    def loadData(self):\n",
        "        # Read the text\n",
        "        input_sequence = []\n",
        "        with open(f\"{self.args.workingDir}/{self.args.inputFile}.csv\", newline='') as csvfile:\n",
        "          spamreader = csv.reader(csvfile, delimiter=',', quotechar='|')\n",
        "          spamreader.__next__()\n",
        "          for row in spamreader:\n",
        "              new_row = []\n",
        "              \n",
        "              # wrap each value in row in an array\n",
        "              for val in row:\n",
        "                new_row.append([float(val)])\n",
        "\n",
        "              # if this is the first row will need to initialize input sequence with arrays\n",
        "              while len(new_row) > len(input_sequence):\n",
        "                input_sequence.append([])\n",
        "              \n",
        "              # put each wrapped value in the correct index \n",
        "              for idx in range(len(new_row)):\n",
        "                input_sequence[idx].append(new_row[idx])\n",
        "        test_len = int(args.test_split * len(input_sequence[0]))\n",
        "        input_sequence = torch.tensor(input_sequence)[:, 0:test_len, :]\n",
        "\n",
        "\n",
        "\n",
        "        output_sequence = []\n",
        "        with open(f\"{self.args.workingDir}/{self.args.outputFile}.csv\", newline='') as csvfile:\n",
        "          spamreader = csv.reader(csvfile, delimiter=',', quotechar='|')\n",
        "          spamreader.__next__()\n",
        "          for row in spamreader:\n",
        "            new_row = []\n",
        "            for val in row:\n",
        "              new_row.append(float(val) * 100)\n",
        "            output_sequence.append(new_row)\n",
        "        output_sequence = torch.tensor(output_sequence)[0:test_len]\n",
        "        return input_sequence, output_sequence\n",
        "\n",
        "    def __len__(self):\n",
        "        # Get the number of sequences for training purpose.\n",
        "        return len(self.input_sequence[0]) - self.args.seqLength\n",
        "\n",
        "    # returns tensor is size [num_features, 1, seq_length]\n",
        "    def __getitem__(self, index):\n",
        "        return_seq = self.input_sequence[:, index:index+self.args.seqLength, :].transpose(1, 2).detach().clone()\n",
        "\n",
        "        return (\n",
        "            return_seq,\n",
        "            self.output_sequence[index+self.args.seqLength -1],\n",
        "        )\n",
        "\n",
        "\n",
        "class ValData(torch.utils.data.Dataset):\n",
        "    def __init__(self,args):\n",
        "        self.args = args\n",
        "        self.input_sequence , self.output_sequence = self.loadData()\n",
        "\n",
        "    def loadData(self):\n",
        "        # Read the text\n",
        "        input_sequence = []\n",
        "        with open(f\"{self.args.workingDir}/{self.args.inputFile}.csv\", newline='') as csvfile:\n",
        "          spamreader = csv.reader(csvfile, delimiter=',', quotechar='|')\n",
        "          spamreader.__next__()\n",
        "          for row in spamreader:\n",
        "              new_row = []\n",
        "              for val in row:\n",
        "                new_row.append([float(val)])\n",
        "              while len(new_row) > len(input_sequence):\n",
        "                input_sequence.append([])\n",
        "              for idx in range(len(new_row)):\n",
        "                input_sequence[idx].append(new_row[idx])\n",
        "        test_len = int(args.test_split * len(input_sequence[0]))\n",
        "        input_sequence = torch.tensor(input_sequence)[:, test_len: , :]\n",
        "\n",
        "        output_sequence = []\n",
        "        with open(f\"{self.args.workingDir}/{self.args.outputFile}.csv\", newline='') as csvfile:\n",
        "          spamreader = csv.reader(csvfile, delimiter=',', quotechar='|')\n",
        "          spamreader.__next__()\n",
        "          for row in spamreader:\n",
        "            new_row = []\n",
        "            for val in row:\n",
        "              new_row.append(float(val) * 100)\n",
        "            output_sequence.append(new_row)\n",
        "        output_sequence = torch.tensor(output_sequence)[test_len: ]\n",
        "        return input_sequence, output_sequence\n",
        "\n",
        "    def __len__(self):\n",
        "        # Get the number of sequences for training purpose.\n",
        "        return len(self.input_sequence[0]) - self.args.seqLength\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        return_seq = self.input_sequence[:, index:index+self.args.seqLength, :].transpose(1, 2).detach().clone()\n",
        "\n",
        "\n",
        "        return (\n",
        "            return_seq,\n",
        "            self.output_sequence[index+self.args.seqLength - 1],\n",
        "        )"
      ],
      "metadata": {
        "id": "9WXROY0Z4Sss"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument('-f')\n",
        "parser.add_argument('--workingDir', type=str, \n",
        "    default=\"/content/drive/My Drive/Neural Networks Project\")\n",
        "# Can be game_of_thrones or wonder_land:\n",
        "parser.add_argument('--inputFile', type=str, default=\"input_data\") \n",
        "parser.add_argument('--outputFile', type=str, default=\"output_data\") \n",
        "parser.add_argument('--maxEpochs', type=int, default=100)\n",
        "parser.add_argument('--batchSize', type=int, default=12)\n",
        "parser.add_argument('--seqLength', type=int, default=63)\n",
        "parser.add_argument('--learningRate', type=float, default=.0001)\n",
        "parser.add_argument('--dropoutRate', type=float, default=0.3)\n",
        "parser.add_argument('--test_split', type=float, default=.9)\n",
        "parser.add_argument('--num_channels', type=float, default=2)\n",
        "args = parser.parse_args()"
      ],
      "metadata": {
        "id": "5GUSyYRanh58"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = TrainData(args)\n",
        "train_loader = DataLoader(dataset,  batch_size=args.batchSize, shuffle=False, drop_last=False, num_workers=2)\n",
        "valset = ValData(args)\n",
        "val_loader = DataLoader(valset, batch_size=args.batchSize, shuffle=False, drop_last=False, num_workers=2)\n",
        "data_iter = iter(train_loader)\n",
        "print(dataset.__len__())\n",
        "print(valset.__len__())"
      ],
      "metadata": {
        "id": "ox0ZhjS7nVO8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2c06a95d-618c-406e-a985-265535628899"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4222\n",
            "414\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### The main training and evaluation code starts here"
      ],
      "metadata": {
        "id": "35KXsx9VGiWx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':\n",
        "    # Specify the operation mode:\n",
        "    # 'train' = training with your train and validation data splits\n",
        "    # 'eval'  = evaluation of the trained model with your test data split \n",
        "    mode = 'train'\n",
        "\n",
        "    # Path where you plan to save the best model during training\n",
        "    my_best_model = \"/content/drive/MyDrive/Convolution_best_model.pth\"\n",
        "\n",
        "    # Set the device (GPU or CPU, depending on availability)\n",
        "    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "    print(\"Currently using device: \", device)\n",
        "\n",
        "    # Initialize the model and print out its configuration\n",
        "    model = CNN(numChannels = args.num_channels)\n",
        "    model.to(device)\n",
        "    print(\"\\n\\nModel summary:\\n\\n\")\n",
        "    summary(model, input_size=(args.num_channels, 1, 63))\n",
        "\n",
        "    if mode == \"train\":\n",
        "\n",
        "        print(\"\\n\\nTraining starts!\\n\\n\")\n",
        "        \n",
        "        model.train()\n",
        "        criterion = nn.MSELoss()\n",
        "        optimizer = torch.optim.Adam(model.parameters(), lr=args.learningRate)\n",
        "        # optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.5)\n",
        "        \n",
        "        best_val_loss = float('inf')\n",
        "        for epoch in range(args.maxEpochs):\n",
        "            running_loss = .0\n",
        "            print(f\"Starting epoch {epoch + 1}\")\n",
        "            bestLoss = float('inf')\n",
        "            for idx, data in tqdm(enumerate(train_loader), total=len(train_loader)):\n",
        "                # Get the inputs (data is a list of [inputs, labels])\n",
        "              \n",
        "                inputs, labels = data\n",
        "                inputs = inputs.to(device)\n",
        "                labels = labels.to(device)\n",
        "                optimizer.zero_grad()\n",
        "                outputs = model(inputs)\n",
        "                \n",
        "                loss = criterion(outputs, labels)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "                loss = loss.detach().cpu().numpy()\n",
        "                inputs = inputs.detach().cpu().numpy()\n",
        "                labels = labels.detach().cpu().numpy()\n",
        "                running_loss += loss\n",
        "              \n",
        "            \n",
        "                currLoss = loss.item()\n",
        "                \n",
        "                # Only save models with smallest loss per epoch.\n",
        "                if currLoss < bestLoss:\n",
        "                    bestLoss = currLoss\n",
        "                    # torch.save(model.state_dict(), my_best_model)\n",
        "            print(f\"Epoch ID: {epoch}, 'the ave loss': {running_loss/ train_loader.__len__()}\")\n",
        "\n",
        "            # Evaluate the accuracy after each epoch\n",
        "            val_loss = model.evaluate(model, val_loader, device)\n",
        "            print(f\"Epoch ID: {epoch}, 'the ave val loss': {val_loss}\")\n",
        "            if val_loss < best_val_loss:\n",
        "                print(f\"Better validation accuracy achieved: {val_loss}\")\n",
        "                best_val_loss = val_loss\n",
        "                print(f\"Saving this model as: {my_best_model}\")\n",
        "                torch.save(model.state_dict(), my_best_model)\n",
        "\n",
        "    # And here we evaluate the trained model with the test data\n",
        "    # elif mode == \"eval\":\n",
        "\n",
        "    #     print(\"\\n\\nValidating the trained model:\")\n",
        "    #     print(f\"Loading checkpoint from {my_best_model}\")\n",
        "    #     model.load_state_dict(torch.load(my_best_model))\n",
        "    #     acc = model.evaluate(model, test_loader, classes, device)\n",
        "    #     print(f\"Accuracy on the test (unknown) data: {acc * 100:.2f}%\")\n",
        "\n",
        "    else:\n",
        "        print(\"'mode' argument should either be 'train' or 'eval'\")"
      ],
      "metadata": {
        "id": "yy5S2JkWgzsj",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "2539ecce-d016-43d3-8b7e-e25322d8070f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Currently using device:  cpu\n",
            "\n",
            "\n",
            "Model summary:\n",
            "\n",
            "\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1             [-1, 7, 1, 54]             147\n",
            "              ReLU-2             [-1, 7, 1, 54]               0\n",
            "         Dropout2d-3             [-1, 7, 1, 54]               0\n",
            "         MaxPool2d-4             [-1, 7, 1, 27]               0\n",
            "            Conv2d-5            [-1, 15, 1, 23]             540\n",
            "              ReLU-6            [-1, 15, 1, 23]               0\n",
            "         Dropout2d-7            [-1, 15, 1, 23]               0\n",
            "         MaxPool2d-8            [-1, 15, 1, 11]               0\n",
            "            Conv2d-9             [-1, 15, 1, 9]             690\n",
            "             ReLU-10             [-1, 15, 1, 9]               0\n",
            "        Dropout2d-11             [-1, 15, 1, 9]               0\n",
            "        MaxPool2d-12             [-1, 15, 1, 4]               0\n",
            "           Linear-13                    [-1, 3]             183\n",
            "================================================================\n",
            "Total params: 1,560\n",
            "Trainable params: 1,560\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.00\n",
            "Forward/backward pass size (MB): 0.02\n",
            "Params size (MB): 0.01\n",
            "Estimated Total Size (MB): 0.03\n",
            "----------------------------------------------------------------\n",
            "\n",
            "\n",
            "Training starts!\n",
            "\n",
            "\n",
            "Starting epoch 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 352/352 [00:02<00:00, 174.43it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch ID: 0, 'the ave loss': 3321.5847975990987\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch ID: 0, 'the ave val loss': 2017.3684779575892\n",
            "Better validation accuracy achieved: 2017.3684779575892\n",
            "Saving this model as: /content/drive/MyDrive/Convolution_best_model.pth\n",
            "Starting epoch 2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 352/352 [00:01<00:00, 185.85it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch ID: 1, 'the ave loss': 2140.9608580849385\n",
            "Epoch ID: 1, 'the ave val loss': 1945.0332903180804\n",
            "Better validation accuracy achieved: 1945.0332903180804\n",
            "Saving this model as: /content/drive/MyDrive/Convolution_best_model.pth\n",
            "Starting epoch 3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 352/352 [00:02<00:00, 150.67it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch ID: 2, 'the ave loss': 1991.1561766537752\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch ID: 2, 'the ave val loss': 1935.7190220424106\n",
            "Better validation accuracy achieved: 1935.7190220424106\n",
            "Saving this model as: /content/drive/MyDrive/Convolution_best_model.pth\n",
            "Starting epoch 4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 352/352 [00:02<00:00, 129.82it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch ID: 3, 'the ave loss': 1986.9526041204278\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch ID: 3, 'the ave val loss': 1926.5802769252232\n",
            "Better validation accuracy achieved: 1926.5802769252232\n",
            "Saving this model as: /content/drive/MyDrive/Convolution_best_model.pth\n",
            "Starting epoch 5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 352/352 [00:01<00:00, 182.38it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch ID: 4, 'the ave loss': 1982.438146764582\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch ID: 4, 'the ave val loss': 1922.1742030552455\n",
            "Better validation accuracy achieved: 1922.1742030552455\n",
            "Saving this model as: /content/drive/MyDrive/Convolution_best_model.pth\n",
            "Starting epoch 6\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 352/352 [00:01<00:00, 180.72it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch ID: 5, 'the ave loss': 1973.938596205278\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch ID: 5, 'the ave val loss': 1922.965576171875\n",
            "Starting epoch 7\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 352/352 [00:01<00:00, 180.10it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch ID: 6, 'the ave loss': 1971.8579455288973\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch ID: 6, 'the ave val loss': 1900.8023472377233\n",
            "Better validation accuracy achieved: 1900.8023472377233\n",
            "Saving this model as: /content/drive/MyDrive/Convolution_best_model.pth\n",
            "Starting epoch 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 352/352 [00:01<00:00, 176.18it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch ID: 7, 'the ave loss': 1962.6812754544344\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch ID: 7, 'the ave val loss': 1890.2595581054688\n",
            "Better validation accuracy achieved: 1890.2595581054688\n",
            "Saving this model as: /content/drive/MyDrive/Convolution_best_model.pth\n",
            "Starting epoch 9\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 352/352 [00:02<00:00, 144.16it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch ID: 8, 'the ave loss': 1959.565510489724\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch ID: 8, 'the ave val loss': 1906.4531284877232\n",
            "Starting epoch 10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 352/352 [00:02<00:00, 128.96it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch ID: 9, 'the ave loss': 1955.5648775967684\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch ID: 9, 'the ave val loss': 1890.7747209821428\n",
            "Starting epoch 11\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 352/352 [00:01<00:00, 176.86it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch ID: 10, 'the ave loss': 1951.6715419075706\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch ID: 10, 'the ave val loss': 1869.1228009905135\n",
            "Better validation accuracy achieved: 1869.1228009905135\n",
            "Saving this model as: /content/drive/MyDrive/Convolution_best_model.pth\n",
            "Starting epoch 12\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 352/352 [00:02<00:00, 173.33it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch ID: 11, 'the ave loss': 1947.252121145075\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch ID: 11, 'the ave val loss': 1883.7446358816965\n",
            "Starting epoch 13\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 352/352 [00:01<00:00, 176.45it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch ID: 12, 'the ave loss': 1943.6783050190318\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch ID: 12, 'the ave val loss': 1853.201438685826\n",
            "Better validation accuracy achieved: 1853.201438685826\n",
            "Saving this model as: /content/drive/MyDrive/Convolution_best_model.pth\n",
            "Starting epoch 14\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 352/352 [00:01<00:00, 181.42it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch ID: 13, 'the ave loss': 1939.5101117220793\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch ID: 13, 'the ave val loss': 1859.000545828683\n",
            "Starting epoch 15\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 352/352 [00:02<00:00, 141.37it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch ID: 14, 'the ave loss': 1935.8964795199308\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch ID: 14, 'the ave val loss': 1852.6616629464286\n",
            "Better validation accuracy achieved: 1852.6616629464286\n",
            "Saving this model as: /content/drive/MyDrive/Convolution_best_model.pth\n",
            "Starting epoch 16\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 352/352 [00:02<00:00, 126.73it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch ID: 15, 'the ave loss': 1930.3825265711005\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch ID: 15, 'the ave val loss': 1841.202068219866\n",
            "Better validation accuracy achieved: 1841.202068219866\n",
            "Saving this model as: /content/drive/MyDrive/Convolution_best_model.pth\n",
            "Starting epoch 17\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 352/352 [00:03<00:00, 92.50it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch ID: 16, 'the ave loss': 1927.022685137662\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch ID: 16, 'the ave val loss': 1842.9866646902901\n",
            "Starting epoch 18\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 352/352 [00:03<00:00, 92.05it/s] "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch ID: 17, 'the ave loss': 1922.6733707081187\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch ID: 17, 'the ave val loss': 1833.8608450753347\n",
            "Better validation accuracy achieved: 1833.8608450753347\n",
            "Saving this model as: /content/drive/MyDrive/Convolution_best_model.pth\n",
            "Starting epoch 19\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 352/352 [00:02<00:00, 139.30it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch ID: 18, 'the ave loss': 1919.8439098704946\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch ID: 18, 'the ave val loss': 1823.5899919782366\n",
            "Better validation accuracy achieved: 1823.5899919782366\n",
            "Saving this model as: /content/drive/MyDrive/Convolution_best_model.pth\n",
            "Starting epoch 20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 352/352 [00:02<00:00, 139.85it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch ID: 19, 'the ave loss': 1916.5298950888894\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch ID: 19, 'the ave val loss': 1819.2513305664063\n",
            "Better validation accuracy achieved: 1819.2513305664063\n",
            "Saving this model as: /content/drive/MyDrive/Convolution_best_model.pth\n",
            "Starting epoch 21\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 352/352 [00:01<00:00, 191.47it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch ID: 20, 'the ave loss': 1913.5984391299162\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch ID: 20, 'the ave val loss': 1814.2085710797992\n",
            "Better validation accuracy achieved: 1814.2085710797992\n",
            "Saving this model as: /content/drive/MyDrive/Convolution_best_model.pth\n",
            "Starting epoch 22\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 84%|████████▍ | 296/352 [00:01<00:00, 188.46it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-d7bdc04692ac>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     32\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Starting epoch {epoch + 1}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m             \u001b[0mbestLoss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'inf'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m                 \u001b[0;31m# Get the inputs (data is a list of [inputs, labels])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tqdm/std.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1177\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1178\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1179\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1180\u001b[0m                 \u001b[0;31m# Update and possibly print the progressbar.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    632\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    633\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 634\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    635\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    636\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1327\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1328\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shutdown\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1329\u001b[0;31m             \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1330\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1331\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1293\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1294\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1295\u001b[0;31m                 \u001b[0msuccess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1296\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0msuccess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1297\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1131\u001b[0m         \u001b[0;31m#   (bool: whether successfully get data, any: data if successful else None)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1133\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1134\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1135\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.9/multiprocessing/queues.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    120\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_rlock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m         \u001b[0;31m# unserialize the data after having released the lock\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_ForkingPickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mqsize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/multiprocessing/reductions.py\u001b[0m in \u001b[0;36mrebuild_tensor\u001b[0;34m(cls, storage, metadata)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0mrebuild_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstorage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m     \u001b[0mstorage_offset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequires_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmetadata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_rebuild_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstorage_offset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstride\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "checkpointFile = \"/content/drive/MyDrive/Convolution_best_model\"\n",
        "\n",
        "# If using GPU, we need the following line.\n",
        "model.to(device)\n",
        "\n",
        "model.load_state_dict(torch.load(f\"{checkpointFile}.pth\"))\n",
        "\n",
        "\n",
        "generatingLoader = DataLoader(valset, batch_size=1, shuffle=True)\n",
        "\n",
        "\n",
        "# Get one data point from the DataLoader\n",
        "data_point, true = next(iter(generatingLoader))\n",
        "data_point = data_point.to(device)\n",
        "print(data_point)\n",
        "print(true)\n",
        "model.eval()\n",
        "torch.no_grad()\n",
        "print(model(data_point))"
      ],
      "metadata": {
        "id": "CBuoY8J3dWwJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1389a92b-d652-4974-bcad-1ea3e8393c62"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[[3921.1599, 3885.5500, 3857.0701, 3873.7100, 3915.8000, 3839.6599,\n",
            "           3842.5100, 3903.6399, 3863.9900, 3818.5300, 3793.5801, 3844.3899,\n",
            "           3851.9299, 3891.9900, 3915.5400, 3924.5200, 3942.9600, 3973.5901,\n",
            "           3949.5701, 3953.5000, 3913.1399, 3916.4800, 3937.6001, 3919.9299,\n",
            "           3879.3401, 3917.1201, 3969.3101, 3963.3401, 3967.2500, 3992.7800,\n",
            "           4034.4399, 4075.5701, 4074.2900, 4089.9500, 4096.1099, 4124.7100,\n",
            "           4130.1001, 4141.5801, 4139.7598, 4174.1401, 4179.7998, 4159.1802,\n",
            "           4128.4199, 4170.4600, 4138.7798, 4185.0298, 4188.2500, 4185.1401,\n",
            "           4206.1401, 4198.1001, 4191.9800, 4179.0400, 4177.0601, 4169.1401,\n",
            "           4210.3398, 4228.2900, 4150.3398, 4130.5498, 4074.9900, 4129.5801,\n",
            "           4169.9199, 4165.9399, 4098.4502]],\n",
            "\n",
            "         [[ 116.0200,  115.7600,  115.7500,  115.0600,  115.0100,  114.9600,\n",
            "            114.9700,  115.0400,  114.8000,  114.7900,  114.1700,  114.2000,\n",
            "            114.2000,  114.3000,  114.4700,  113.8800,  113.6000,  113.9700,\n",
            "            113.5500,  113.3200,  113.4000,  113.7000,  113.8900,  113.9000,\n",
            "            114.2400,  113.9100,  113.5200,  113.6400,  113.8400,  113.9300,\n",
            "            113.7800,  113.9800,  114.1200,  114.1800,  114.1100,  114.1700,\n",
            "            114.1200,  114.4100,  113.7800,  114.5600,  114.0000,  114.4200,\n",
            "            114.6000,  114.7400,  114.8100,  114.7400,  115.1300,  114.4500,\n",
            "            114.1400,  114.4100,  114.4000,  114.5500,  114.4500,  114.5500,\n",
            "            114.8500,  114.6100,  114.2300,  113.9800,  113.9000,  114.2200,\n",
            "            114.2100,  114.1100,  114.6200]]]])\n",
            "tensor([[100.0000,   0.0000,   9.0052]])\n",
            "tensor([[70.5977, 33.3958, 51.4456]], grad_fn=<AddmmBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# detemine how much money I would make\n",
        "class TestData(torch.utils.data.Dataset):\n",
        "    def __init__(self,args):\n",
        "        self.args = args\n",
        "        self.input_sequence , self.output_sequence = self.loadData()\n",
        "\n",
        "    def loadData(self):\n",
        "        # Read the text\n",
        "        input_sequence = []\n",
        "        with open(f\"{self.args.workingDir}/{self.args.inputFile}.csv\", newline='') as csvfile:\n",
        "          spamreader = csv.reader(csvfile, delimiter=',', quotechar='|')\n",
        "          spamreader.__next__()\n",
        "          for row in spamreader:\n",
        "              new_row = []\n",
        "              for val in row:\n",
        "                new_row.append([float(val)])\n",
        "              while len(new_row) > len(input_sequence):\n",
        "                input_sequence.append([])\n",
        "              for idx in range(len(new_row)):\n",
        "                input_sequence[idx].append(new_row[idx])\n",
        "        test_len = int(args.test_split * len(input_sequence[0]))\n",
        "        # input_sequence = torch.tensor(input_sequence)[:, test_len: , :]\n",
        "        input_sequence = torch.tensor(input_sequence)[:, : , :]\n",
        "\n",
        "        output_sequence = []\n",
        "        with open(f\"{self.args.workingDir}/{self.args.outputFile}.csv\", newline='') as csvfile:\n",
        "          spamreader = csv.reader(csvfile, delimiter=',', quotechar='|')\n",
        "          spamreader.__next__()\n",
        "          for row in spamreader:\n",
        "            new_row = []\n",
        "            for val in row:\n",
        "              new_row.append(float(val) * 100)\n",
        "            output_sequence.append(new_row)\n",
        "        # output_sequence = torch.tensor(output_sequence)[test_len: ]\n",
        "        output_sequence = torch.tensor(output_sequence)[: ]\n",
        "\n",
        "        return input_sequence, output_sequence\n",
        "\n",
        "    def __len__(self):\n",
        "        # Get the number of sequences for training purpose.\n",
        "        return len(self.input_sequence[0]) - self.args.seqLength\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        \n",
        "        return_seq = self.input_sequence[:, index:index+self.args.seqLength, :].transpose(1, 2).detach().clone()\n",
        "        values = self.input_sequence[:, index+self.args.seqLength + 21, :].detach().clone()\n",
        "        return (\n",
        "            values,\n",
        "            return_seq,\n",
        "            self.output_sequence[index+self.args.seqLength - 1],\n",
        "        )\n",
        "\n",
        "model.eval()\n",
        "torch.no_grad()\n",
        "testset = TestData(args)\n",
        "train_loader = DataLoader(testset,  batch_size=args.batchSize, shuffle=False, drop_last=False, num_workers=2)\n",
        "\n",
        "total_ret = 1\n",
        "base_ret = 1\n",
        "idx = 0\n",
        "while(idx < testset.__len__() - 21):\n",
        "    values, data_point, _ = testset.__getitem__(idx)\n",
        "    data_point = data_point.unsqueeze(0)\n",
        "    data_point = data_point.to(device)\n",
        "    outputs = model(data_point)\n",
        "    outputs = outputs.squeeze()\n",
        "    # print(sum(outputs))\n",
        "    outputs /= sum(outputs)\n",
        "    data_point = data_point.squeeze()\n",
        "    values = values.squeeze()\n",
        "    values = values.to(device)\n",
        "    last_pt = data_point[:, -1]\n",
        "    percentage = (values - last_pt) / last_pt\n",
        "    gains = []\n",
        "    for i in range(2):\n",
        "      gains.append(percentage[i] * outputs[i])\n",
        "    # outputs *= per\n",
        "    gains.append(.00575 * outputs[2])\n",
        "    for i in range(len(gains)):\n",
        "      gains[i] = gains[i].item()\n",
        "    tot_gains = sum(gains)\n",
        "    # print(tot_gains)\n",
        "    total_ret = total_ret + (tot_gains * total_ret)\n",
        "    base_ret = base_ret + (base_ret * ((values[0].item() - last_pt[0].item())/ last_pt[0].item()))\n",
        "    idx += 21\n",
        "\n",
        "print(total_ret)\n",
        "print(base_ret)\n",
        "\n",
        "# input_sequence = []\n",
        "# with open(f\"{args.workingDir}/{args.inputFile}.csv\", newline='') as csvfile:\n",
        "#   spamreader = csv.reader(csvfile, delimiter=',', quotechar='|')\n",
        "#   spamreader.__next__()\n",
        "#   for row in spamreader:\n",
        "#       new_row = []\n",
        "#       for val in row:\n",
        "#         new_row.append([float(val)])\n",
        "#       while len(new_row) > len(input_sequence):\n",
        "#         input_sequence.append([])\n",
        "#       for idx in range(len(new_row)):\n",
        "#         input_sequence[idx].append(new_row[idx])\n",
        "# test_len = int(args.test_split * len(input_sequence[0]))\n",
        "# input_sequence = torch.tensor(input_sequence)[:, test_len: , :]\n",
        "# print(input_sequence.shape)\n",
        "\n",
        "# idx = 0\n",
        "# while(idx == 0):\n",
        "#   input = input_sequence[:, idx : idx + args.seqLength, :]\n",
        "#   input = input.transpose(1, 2)\n",
        "#   print(input.shape)\n",
        "#   idx+= 1\n",
        "#   model.eval()\n",
        "#   torch.no_grad()\n",
        "#   print(model(data_point))\n",
        "\n"
      ],
      "metadata": {
        "id": "YzYhq6MLzKH_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0618b0d1-e471-4560-838c-43c1e538415f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3.118867308213026\n",
            "4.2588536379158946\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "w2HsG18WgLtl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}