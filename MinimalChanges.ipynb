{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/plantehenry/NeuralNetworksFinalProject/blob/main/MinimalChanges.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Introduction to Neural Networks (CSE 40868/60868)\n",
        "# University of Notre Dame\n",
        "# Practical 4: Text generation using LSTM and GRU\n",
        "# _________________________________________________________________________\n",
        "# Qi Li (lead programmer), Adam Czajka (\"destroyer\"), March 2022"
      ],
      "metadata": {
        "id": "gdli1xfQXiMN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Prerequisites:\n",
        "\n",
        "1. Copy this notebook to your Google Drive\n",
        "2. Create a folder in your Google Drive for this practical\n",
        "3. Create a `data` subfolder and upload the \"Wonder Land\" text there\n",
        "\n",
        "\n",
        "### Tasks for today:\n",
        "\n",
        "**Task 1 (2 points).** Load the data, do a short training of the the LSTM, and generate some example text (50 words is enough). Paste your generated text below.\n",
        "\n",
        "**Task 2 (1 point).** Making some useful data curation steps, retraining the LSTM and generating a sample text (again, 50 words is enough). Paste your generated text below.\n",
        "\n",
        "**Task 3 (2 points).** Switching to GRU and comparing it with LSTM (and training both models for a bit longer to get reasonable output). Paste your generated texts (for GRU and LSTM) below. Do you see any differences in the generated texts? Is any text qualitatively better than the other? If so, how would you explain the reasons for the observed differences?\n",
        "\n",
        "**Task 4 (for 60868 section attendees).** Train either LSTM or GRU on a text from your researcgh area (for instance, a research paper or part of it). Experiment a bit with the loss (cross entropy vs MSE), optimizer (Adam, SGD, etc.) and their parameters. Since it's your research area, you should be able to assess how reasonable the outputs are. Is there any configuration (model / loss / parameters) that is better than others? If so, try to explain why. Have some fun with text generation!\n",
        "\n",
        "### After completing the above tasks:\n",
        "\n",
        "1. Discuss your solutions and observations with Qi, Ning, Qingkai or Adam in class.\n",
        "2. Share your Google Colab notebook with all at Notre Dame (for reading). Please! Please! Remember about this step!\n",
        "3. Submit the link via Canvas as your Practical 4 submission.\n",
        "4. You may add any additional comments below -- we will be happy to read and discuss your observations!\n",
        "### Your solutions:\n",
        "\n",
        "Example text generated in Task 1:\n",
        "> ...\n",
        "\n",
        "Example text generated in Task 2:\n",
        "> ...\n",
        "\n",
        "Example texts generated in Task 3:\n",
        "> ...\n",
        "\n",
        "Example texts generated in Task 4 (60868):\n",
        "> ...\n",
        "\n",
        "Your comments:\n",
        "...\n"
      ],
      "metadata": {
        "id": "jcHEFeDJN49V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Starting with some imports, as usual"
      ],
      "metadata": {
        "id": "ouLx09o0uqtU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch import nn, optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data.dataloader import default_collate\n",
        "import sys\n",
        "import os\n",
        "from collections import Counter\n",
        "import string\n",
        "import numpy as np\n",
        "import argparse"
      ],
      "metadata": {
        "id": "nX3hA54pczDz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Mount your Google Drive"
      ],
      "metadata": {
        "id": "J3lCaDNo2Zqn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "IsMl3PZI1V9w",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1fe9e070-35c0-4590-e2f6-5fbaac0004b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Here we construct our data loader"
      ],
      "metadata": {
        "id": "w2xpT6uv2XwI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "class LoadData(torch.utils.data.Dataset):\n",
        "    def __init__(self,args):\n",
        "        self.args = args\n",
        "        self.sequence = self.loadText()\n",
        "\n",
        "        \n",
        "        # Data processing, mapping each unique word to an integer.\n",
        "\n",
        "\n",
        "    def loadText(self):\n",
        "        input_sequence = []\n",
        "        with open(f\"{self.args.workingDir}/{self.args.inputFile}.csv\", newline='') as csvfile:\n",
        "          spamreader = csv.reader(csvfile, delimiter=',', quotechar='|')\n",
        "          spamreader.__next__()\n",
        "          for row in spamreader:\n",
        "              new_row = float(row[0])\n",
        "              # for val in row:\n",
        "                # new_row.append(float(val))\n",
        "              input_sequence.append(new_row)\n",
        "\n",
        "       \n",
        "        ret_array = []\n",
        "        for val in input_sequence:\n",
        "          ret_array.append((val - min(input_sequence))/ (max(input_sequence) - min(input_sequence)))\n",
        "\n",
        "        for idx in range(len(ret_array)):\n",
        "          ret_array[idx] = [ret_array[idx]]\n",
        "        \n",
        "        return ret_array\n",
        "        # output_sequence = []\n",
        "        # with open(f\"{self.args.workingDir}/{self.args.outputFile}.csv\", newline='') as csvfile:\n",
        "        #   spamreader = csv.reader(csvfile, delimiter=',', quotechar='|')\n",
        "        #   spamreader.__next__()\n",
        "        #   for row in spamreader:\n",
        "        #     new_row = []\n",
        "        #     for val in row:\n",
        "        #       new_row.append(float(val))\n",
        "        #     output_sequence.append(new_row)\n",
        "        \n",
        "        # return input_sequence, output_sequence\n",
        "  \n",
        "\n",
        "    def __len__(self):\n",
        "        # Get the number of sequences for training purpose.\n",
        "        return len(self.sequence) - self.args.seqLength\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        return (\n",
        "            torch.tensor(self.sequence[index:index+self.args.seqLength]),\n",
        "            torch.tensor(self.sequence[index+1:index+self.args.seqLength+1]),\n",
        "        )"
      ],
      "metadata": {
        "id": "FLN5PXGp88sr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Next, we define our Recurrent Neural Network (either LSTM or GRU, depending on the `rnnType` argument)"
      ],
      "metadata": {
        "id": "kn98KFxIxcd2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Model(nn.Module):\n",
        "    def __init__(self, dataset, args):\n",
        "        super(Model, self).__init__()\n",
        "\n",
        "        self.batch_size = args.batchSize\n",
        "\n",
        "        # Define input dimension of RNN.\n",
        "        self.inputSize = 1\n",
        "        \n",
        "        # Define embedding dimension of the RNN, i.e., output size of a RNN layer.\n",
        "        self.embeddingDim = 256\n",
        "\n",
        "        # Define the number of layers of the RNN.\n",
        "        self.numLayers = 6\n",
        "\n",
        "        # Find the number of unique words in input file.\n",
        "        # numWords = len(dataset.uniqueWords)\n",
        "\n",
        "        # # Define the embedding function.\n",
        "        # self.embedding = nn.Embedding(num_embeddings=numWords,\n",
        "        #     embedding_dim=self.embeddingDim,)\n",
        "  \n",
        "\n",
        "\n",
        "        #######################################################################\n",
        "        # *** Build your RNN units for both models: peephole LSTM and GRU.\n",
        "        # Look at the Pytorch documentation:\n",
        "        # -- LSTM: https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html\n",
        "        # -- GRU: https://pytorch.org/docs/stable/generated/torch.nn.GRU.html\n",
        "        #\n",
        "        # Define (using variables defined above in this class):\n",
        "        # -- \"input_size\"\n",
        "        # -- \"hidden_size\"\n",
        "        # -- \"num_layers\"\n",
        "        # -- \"dropout\" rate that comes in \"args.dropoutRate\"        \n",
        "        if args.rnnType == \"LSTM\":\n",
        "            self.rnnUnit = nn.LSTM(input_size=self.inputSize,\n",
        "                hidden_size=self.inputSize, num_layers=self.numLayers,\n",
        "                dropout=args.dropoutRate)\n",
        "        else: # GRU\n",
        "            self.rnnUnit = nn.GRU(input_size=self.inputSize, \n",
        "                hidden_size=self.inputSize, num_layers=self.numLayers,\n",
        "                dropout=args.dropoutRate)\n",
        "        #######################################################################\n",
        "\n",
        "        # Fully-connected layer generating our output.\n",
        "        self.fc = nn.Linear(self.inputSize, 1)\n",
        "        # self.fc = nn.Linear(self.embeddingDim, 1)\n",
        "\n",
        "    def forward(self, X, prevState):\n",
        "        \n",
        "        # Calculate embeddings.\n",
        "        # embedding = self.embedding(X)\n",
        "\n",
        "        # Get the output from the RNN cell (either LSTM or GRU).\n",
        "        output, state = self.rnnUnit(X, prevState)\n",
        "\n",
        "        # And pass it through the FC layer to get the RNN's output for a given time step.\n",
        "        logits = self.fc(output)\n",
        "\n",
        "        return logits, state\n",
        "\n",
        "    def initState(self, seqLength):\n",
        "\n",
        "        # Define state initialization for LSTM and GRU.\n",
        "        if args.rnnType == \"LSTM\":\n",
        "            stateHidden = torch.zeros(self.numLayers, seqLength, self.inputSize)\n",
        "            stateCurrent = torch.zeros(self.numLayers, seqLength, self.inputSize)\n",
        "            return (stateHidden, stateCurrent)\n",
        "        # GRU has fewer gates compared to LSTM, we assign 0 as dummy for current state.\n",
        "        else: \n",
        "            weight = next(self.parameters()).data\n",
        "            stateHidden = weight.new(self.numLayers, seqLength, self.embeddingDim).zero_()\n",
        "            return (stateHidden, 0)"
      ],
      "metadata": {
        "id": "xYkquB-6xZzs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training section\n",
        "Estimated training times on GPU: around 25 seconds per epoch (for both LSTM and GRU)."
      ],
      "metadata": {
        "id": "oWe5stfZX1HW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Training the model and generating texts\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    # Parsing arguments. The args has been treated as input for class \"LoadData\" and class \"Model\". \n",
        "    # To run it in the jupyter notebook, please modify the value in default.\n",
        "    # \"Wonder Land\" does not require GPU, CPU is feasible.\n",
        "    # If you are interested in \"Game of Thrones\", or other larger text files, you need GPU.\n",
        "    # If you ran out of GPU time in Google Colab, you may want to switch to a different Google account\n",
        "    # or use CRC resources.\n",
        "    #  \n",
        "    # The steps of running Practical 4 on CRC GPU clusters: \n",
        "    #    1. Fill in your scripts for each task. \n",
        "    #    2. Copy each piece of scripts in this file and create a '.py' file. \n",
        "    #    3. Follow instructions of Practical 1 how to run '.py' code on CRC machines.\n",
        "    #    4. Submit your notebook via Canvas.\n",
        "\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument('-f')\n",
        "    parser.add_argument('--workingDir', type=str, \n",
        "        default=\"/content/drive/My Drive/Neural Networks Project\")\n",
        "    # Can be game_of_thrones or wonder_land:\n",
        "    parser.add_argument('--inputFile', type=str, default=\"input_data\") \n",
        "    parser.add_argument('--maxEpochs', type=int, default=1)\n",
        "    parser.add_argument('--batchSize', type=int, default=128)\n",
        "    parser.add_argument('--seqLength', type=int, default=30)\n",
        "    parser.add_argument('--learningRate', type=float, default=.01)\n",
        "    parser.add_argument('--dropoutRate', type=float, default=0.0)\n",
        "    # Here define which RNN model, i.e., LSTM or GRU you like to train:\n",
        "    parser.add_argument('--rnnType', type=str, default=\"LSTM\") \n",
        "    args = parser.parse_args()\n",
        "\n",
        "    # Create folder for saving your checkpoints.\n",
        "    try:\n",
        "        os.mkdir(f\"{args.workingDir}/checkpoints\") \n",
        "    except:\n",
        "        pass\n",
        "\n",
        "    # If using GPU, we need the following line. Also, please make sure to send \n",
        "    # both models, hidden states, current states, and data to GPU, if you are \n",
        "    # using GPU (to train on \"Game of Thrones\" or other larger texts, you really need GPU).\n",
        "    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "    print(\"currently using: \", device)\n",
        "\n",
        "    dataset = LoadData(args)\n",
        "    model = Model(dataset, args)\n",
        "    model.to(device) # if you're using GPU\n",
        "\n",
        "    def train(dataset, model, args):\n",
        "\n",
        "        # Switch the model to training mode.\n",
        "        model.train()\n",
        "        \n",
        "        # If you are using GPU:\n",
        "        dataloader = DataLoader(dataset, batch_size=args.batchSize, shuffle=True, \n",
        "                    collate_fn=lambda x: tuple(x_.to(device) for x_ in default_collate(x)))\n",
        "        \n",
        "        # If you are using CPU:\n",
        "        # dataloader = DataLoader(dataset, batch_size=args.batchSize, shuffle=True,\n",
        "        #            collate_fn=lambda x: tuple(x_ for x_ in default_collate(x)))\n",
        "        \n",
        "        # Define the loss function and the optimizer. Do these names ring a bell? \n",
        "        # We used exactly the same losses and optimizers in CNNs and MLPs.\n",
        "        # criterion = nn.CrossEntropyLoss()\n",
        "        criterion = nn.MSELoss()\n",
        "        optimizer = optim.Adam(model.parameters(), lr=args.learningRate)\n",
        "\n",
        "        # Training loop starts here.\n",
        "        for epoch in range(1, args.maxEpochs+1):\n",
        "            stateHidden, stateCurrent = model.initState(args.seqLength)\n",
        "            stateHidden = stateHidden.to(device)\n",
        "\n",
        "            #######################################################################\n",
        "            # *** Task 3 *** The line below is only for LSTM\n",
        "            # Comments this line out for GRU (GRU does not pass the \"stateCurrent\" to the next step)\n",
        "            stateCurrent = stateCurrent.to(device)\n",
        "            #######################################################################\n",
        "\n",
        "\n",
        "            bestLoss = float('inf')\n",
        "            for batch, (X, y) in enumerate(dataloader): \n",
        "\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                #######################################################################\n",
        "                # *** Task 1 *** Implement states update in our RNN models.\n",
        "                # Hint: GRU takes only the hidden state, while LSTM both hidden and current states.\n",
        "                if args.rnnType == \"LSTM\":\n",
        "                    # y_pred, (???,???) = model(X, (???,???))\n",
        "                    # print(X.size())\n",
        "                    # print(y.size())\n",
        "                    y_pred, (stateHidden, stateCurrent) = model(X, (stateHidden, stateCurrent))\n",
        "                else: # i.e., GRU\n",
        "                    # y_pred, ??? = model(X, ???)\n",
        "                    y_pred, stateHidden = model(X, stateHidden)\n",
        "                #######################################################################\n",
        "\n",
        "                loss = criterion(y_pred, y)\n",
        "\n",
        "                # Update states for the RNN models. Note the difference between LSTM and GRU!\n",
        "                if args.rnnType == \"LSTM\":\n",
        "                    stateHidden = stateHidden.detach()\n",
        "                    stateCurrent = stateCurrent.detach()\n",
        "                else:\n",
        "                    stateHidden = stateHidden.data\n",
        "\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "                currLoss = loss.item()\n",
        "                \n",
        "                # Only save models with smallest loss per epoch.\n",
        "                if currLoss < bestLoss:\n",
        "                    bestLoss = currLoss\n",
        "                    torch.save(model.state_dict(), f'{args.workingDir}/checkpoints/{args.rnnType}-{args.inputFile}-epoch_{epoch}.pth')\n",
        "            print(f\"Epoch ID: {epoch}, 'the best loss': {bestLoss}\")\n",
        " \n",
        "\n",
        "    # We are ready to train our RNN model!\n",
        "    train(dataset, model, args)\n"
      ],
      "metadata": {
        "id": "8BvICTeZCypW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3180163c-6bc4-47b5-b24d-dd5e2951389f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "currently using:  cuda:0\n",
            "Epoch ID: 1, 'the best loss': 0.050369709730148315\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Ready to generate some text? Let's do it here!\n",
        "First, define our text generator function:"
      ],
      "metadata": {
        "id": "ekapaUgAx8oT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def textGenerator(dataset, idx, model, workingDir, seedSequence, checkpointFileName, outputSize):\n",
        "      checkpointFile = f\"{workingDir}/checkpoints/{checkpointFileName}\"\n",
        "      print (f\"Loading checkpoints from {checkpointFile}.pth \\n\")\n",
        "\n",
        "      # If using GPU, we need the following line.\n",
        "      model.to(device)\n",
        "\n",
        "      # Loading the selected checkpoint (= RNN's weights)\n",
        "      model.load_state_dict(torch.load(f\"{checkpointFile}.pth\"))\n",
        "      model.eval()\n",
        "\n",
        "      # Initialization.\n",
        "      stateHidden, stateCurrent = model.initState(len(seedSequence))\n",
        "\n",
        "      # If using GPU, we need the following line.\n",
        "      stateHidden = stateHidden.to(device)\n",
        "\n",
        "      #######################################################################\n",
        "      # *** Task 3 *** The line below is only for LSTM\n",
        "      stateCurrent = stateCurrent.to(device)  # comment this line for GRU\n",
        "      #######################################################################\n",
        "\n",
        "      for i in range(outputSize):\n",
        "          X = torch.tensor([[[w] for w in seedSequence[i:]]])\n",
        "\n",
        "          # If using GPU, we need the following line.\n",
        "          X = X.to(device)\n",
        "\n",
        "          #######################################################################\n",
        "          # *** Task 1 *** Implement states update in our RNN models (as you did above already).\n",
        "          if args.rnnType == \"LSTM\":\n",
        "              # y_pred, (???,???) = model(X, (???,???))\n",
        "              y_pred, (stateHidden, stateCurrent) = model(X, (stateHidden, stateCurrent))\n",
        "          else: # i.e., GRU\n",
        "              # y_pred, ??? = model(X, ???)\n",
        "              y_pred, stateHidden = model(X, stateHidden)\n",
        "\n",
        "          outputLogit = y_pred[0][-1]\n",
        "          # Note that, even if you are using GPU, numpy can only be run on \n",
        "          # CPU, so if you are using GPU, please remember to send outputLogit to CPU.\n",
        "          # outputLogit = outputLogit.cpu()\n",
        "          # p = torch.nn.functional.softmax(outputLogit, dim=0).detach().numpy()\n",
        "\n",
        "          # # Below we take the most probable word as our output. \n",
        "          # # Note that \"p\" is provided to \"np.random.choice\" so this numpy function will take softmax scores into account when sampling the output\n",
        "          # # Look at https://numpy.org/doc/stable/reference/random/generated/numpy.random.choice.html\n",
        "          # # to better understand this construction \n",
        "          # wordIndex = np.random.choice(len(outputLogit), p=p)\n",
        "\n",
        "          # Output next word:\n",
        "          outputWord = outputLogit.item()\n",
        "          # print(\"on3\")\n",
        "          print(\"___________\")\n",
        "          print(outputWord)\n",
        "          idx += 1\n",
        "          print(dataset.sequence[idx])\n",
        "          print()\n",
        "          # print(outputWord.size())\n",
        "          # sys.stdout.write(outputWord)\n",
        "          # print(\"two\")\n",
        "\n",
        "          # Making sure that each line has \"words_per_line\" words:\n",
        "          words_per_line = 20\n",
        "          if i%words_per_line == 0:\n",
        "              sys.stdout.write(\"\\n\")\n",
        "          else:\n",
        "              sys.stdout.write(\" \")\n",
        "\n",
        "          seedSequence.append(outputWord) "
      ],
      "metadata": {
        "id": "D7vWWrjTcR9g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, select the seed sequence and let it generate a text composed of `outputSize` words"
      ],
      "metadata": {
        "id": "JRztEhxuyLws"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# First, we need to define seed sequence for text generation.\n",
        "# The model will generates texts starting with a sequence of words that the model has seen before, \n",
        "# i.e., in the training stage.\n",
        "seed = np.random.choice(len(dataset.sequence)-1)\n",
        "seedSequence = dataset.sequence[seed:seed+args.seqLength]\n",
        "for idx in range(len(seedSequence)):\n",
        "  seedSequence[idx] = seedSequence[idx][0]\n",
        "print(f\"Seed: {seedSequence}\\n\")\n",
        "\n",
        "# Select the model you want to use:\n",
        "args.rnnType = 'LSTM'\n",
        "\n",
        "# And the number of epochs it was trained for (double check you have this checkpoint\n",
        "# in your Google Drive). Usually you should select the checkpoint with the model offering \n",
        "# the smallest loss. But you can also switch between checkpoints to see if there is any obvious\n",
        "# difference in the generated texts between models offering high and low loss values.\n",
        "training_epochs = 200\n",
        "\n",
        "# How many words to generate?\n",
        "outputSize = 200\n",
        "\n",
        "print (\"!!!!!!! Text generation Starts !!!!!!!\\n\")\n",
        "textGenerator(dataset, seed+args.seqLength , model, args.workingDir, seedSequence, \n",
        "    f\"{args.rnnType}-{args.inputFile}-epoch_{training_epochs}\", outputSize=outputSize)\n",
        "print (\"\\n \\n!!!!!!! Text generation Ends !!!!!!!\\n\")"
      ],
      "metadata": {
        "id": "oMbZZFxjXQ8D",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1f76be23-743d-4f0a-ca9e-fe9d093f6554"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Seed: [0.28465082341874054, 0.24206882827905182, 0.25826886084193135, 0.2924387787714146, 0.30556041318676014, 0.3220167276718144, 0.3229544179988302, 0.3397514366863049, 0.30459257204537094, 0.29267094004209054, 0.30966996918586776, 0.32877352517291497, 0.3771263861836908, 0.3609504742723101, 0.3886862084144892, 0.3903354579607195, 0.397161602334879, 0.3943093352951463, 0.39542491542696573, 0.4084169013405051, 0.4093787123190197, 0.39104400209849666, 0.39197264718120034, 0.3987656256595491, 0.3994349737386406, 0.41210133085694645, 0.4287777462869272, 0.4313405655086744, 0.4350943418981746, 0.41645510845247935]\n",
            "\n",
            "!!!!!!! Text generation Starts !!!!!!!\n",
            "\n",
            "Loading checkpoints from /content/drive/My Drive/Neural Networks Project/checkpoints/LSTM-input_data-epoch_200.pth \n",
            "\n",
            "___________\n",
            "0.31096377968788147\n",
            "[0.41639179174229496]\n",
            "\n",
            "\n",
            "___________\n",
            "0.3937872648239136\n",
            "[0.42069129787195547]\n",
            "\n",
            " ___________\n",
            "0.3588946759700775\n",
            "[0.41921993813052894]\n",
            "\n",
            " ___________\n",
            "0.34057557582855225\n",
            "[0.42843704208450667]\n",
            "\n",
            " ___________\n",
            "0.33078324794769287\n",
            "[0.4304360410774695]\n",
            "\n",
            " ___________\n",
            "0.32466256618499756\n",
            "[0.437684296852858]\n",
            "\n",
            " ___________\n",
            "0.3203502595424652\n",
            "[0.4154812371482154]\n",
            "\n",
            " ___________\n",
            "0.31698763370513916\n",
            "[0.39397767633703784]\n",
            "\n",
            " ___________\n",
            "0.3141498565673828\n",
            "[0.4046540797066928]\n",
            "\n",
            " ___________\n",
            "0.3116125464439392\n",
            "[0.4299536280474936]\n",
            "\n",
            " ___________\n",
            "0.3092483878135681\n",
            "[0.44042500587940886]\n",
            "\n",
            " ___________\n",
            "0.3069828152656555\n",
            "[0.441944606923833]\n",
            "\n",
            " ___________\n",
            "0.30476829409599304\n",
            "[0.44686521982958755]\n",
            "\n",
            " ___________\n",
            "0.30257362127304077\n",
            "[0.44026219148179196]\n",
            "\n",
            " ___________\n",
            "0.3003796637058258\n",
            "[0.4571556927752618]\n",
            "\n",
            " ___________\n",
            "0.29817360639572144\n",
            "[0.46064414199827536]\n",
            "\n",
            " ___________\n",
            "0.2959471642971039\n",
            "[0.4699788341283098]\n",
            "\n",
            " ___________\n",
            "0.2936970591545105\n",
            "[0.4635144995266322]\n",
            "\n",
            " ___________\n",
            "0.2914212942123413\n",
            "[0.4676180253628651]\n",
            "\n",
            " ___________\n",
            "0.2891193926334381\n",
            "[0.4754572370999741]\n",
            "\n",
            " ___________\n",
            "0.28679361939430237\n",
            "[0.48574469496421097]\n",
            "\n",
            "\n",
            "___________\n",
            "0.28444620966911316\n",
            "[0.48956178806389555]\n",
            "\n",
            " ___________\n",
            "0.28208041191101074\n",
            "[0.5053246338183595]\n",
            "\n",
            " ___________\n",
            "0.27970150113105774\n",
            "[0.5162030476443169]\n",
            "\n",
            " ___________\n",
            "0.27731361985206604\n",
            "[0.5202432567703654]\n",
            "\n",
            " ___________\n",
            "0.27492237091064453\n",
            "[0.5202734075847388]\n",
            "\n",
            " ___________\n",
            "0.2725333273410797\n",
            "[0.49317084054440313]\n",
            "\n",
            " ___________\n",
            "0.2701524496078491\n",
            "[0.4773446780797549]\n",
            "\n",
            " ___________\n",
            "0.2677852213382721\n",
            "[0.4540441287319171]\n",
            "\n",
            " ___________\n",
            "0.26543739438056946\n",
            "[0.4954231063781033]\n",
            "\n",
            " ___________\n",
            "0.2631155550479889\n",
            "[0.49696984315546366]\n",
            "\n",
            " ___________\n",
            "0.2608242332935333\n",
            "[0.48657082727804474]\n",
            "\n",
            " ___________\n",
            "0.2585698366165161\n",
            "[0.498224117033401]\n",
            "\n",
            " ___________\n",
            "0.256356418132782\n",
            "[0.48439393848027834]\n",
            "\n",
            " ___________\n",
            "0.2541905343532562\n",
            "[0.4977447190848624]\n",
            "\n",
            " ___________\n",
            "0.2520751655101776\n",
            "[0.4904180711921029]\n",
            "\n",
            " ___________\n",
            "0.2500140070915222\n",
            "[0.4699758190468724]\n",
            "\n",
            " ___________\n",
            "0.24801211059093475\n",
            "[0.47799593567022236]\n",
            "\n",
            " ___________\n",
            "0.24607132375240326\n",
            "[0.46153057594085617]\n",
            "\n",
            " ___________\n",
            "0.2441941201686859\n",
            "[0.47106124836431823]\n",
            "\n",
            " ___________\n",
            "0.24238193035125732\n",
            "[0.48786128213323043]\n",
            "\n",
            "\n",
            "___________\n",
            "0.24063827097415924\n",
            "[0.49923416931491316]\n",
            "\n",
            " ___________\n",
            "0.23896317183971405\n",
            "[0.5027467391894255]\n",
            "\n",
            " ___________\n",
            "0.23735730350017548\n",
            "[0.5061085549920703]\n",
            "\n",
            " ___________\n",
            "0.23582081496715546\n",
            "[0.5020773911103339]\n",
            "\n",
            " ___________\n",
            "0.23435406386852264\n",
            "[0.5090422292306115]\n",
            "\n",
            " ___________\n",
            "0.23295584321022034\n",
            "[0.5018964862240929]\n",
            "\n",
            " ___________\n",
            "0.23162579536437988\n",
            "[0.5177588296659892]\n",
            "\n",
            " ___________\n",
            "0.23036235570907593\n",
            "[0.4984713537112638]\n",
            "\n",
            " ___________\n",
            "0.2291637361049652\n",
            "[0.52406034987005]\n",
            "\n",
            " ___________\n",
            "0.22803010046482086\n",
            "[0.51874777637744]\n",
            "\n",
            " ___________\n",
            "0.22695833444595337\n",
            "[0.5235266804556391]\n",
            "\n",
            " ___________\n",
            "0.2259461134672165\n",
            "[0.5235508011071379]\n",
            "\n",
            " ___________\n",
            "0.224991574883461\n",
            "[0.5368865063045353]\n",
            "\n",
            " ___________\n",
            "0.22409360110759735\n",
            "[0.5327679050611157]\n",
            "\n",
            " ___________\n",
            "0.22324901819229126\n",
            "[0.5378272117129883]\n",
            "\n",
            " ___________\n",
            "0.22245590388774872\n",
            "[0.5218291896064112]\n",
            "\n",
            " ___________\n",
            "0.22171135246753693\n",
            "[0.5222090898675174]\n",
            "\n",
            " ___________\n",
            "0.22101335227489471\n",
            "[0.5265598523816128]\n",
            "\n",
            " ___________\n",
            "0.220359668135643\n",
            "[0.5244342199682812]\n",
            "\n",
            " ___________\n",
            "0.21974778175354004\n",
            "[0.5258030669408381]\n",
            "\n",
            "\n",
            "___________\n",
            "0.21917617321014404\n",
            "[0.5374684170219437]\n",
            "\n",
            " ___________\n",
            "0.2186415195465088\n",
            "[0.5428382770618635]\n",
            "\n",
            " ___________\n",
            "0.21814271807670593\n",
            "[0.5433387805804635]\n",
            "\n",
            " ___________\n",
            "0.21767692267894745\n",
            "[0.5516151791259881]\n",
            "\n",
            " ___________\n",
            "0.21724271774291992\n",
            "[0.5533639263596509]\n",
            "\n",
            " ___________\n",
            "0.21683837473392487\n",
            "[0.5584533838258972]\n",
            "\n",
            " ___________\n",
            "0.21646162867546082\n",
            "[0.5632744990442191]\n",
            "\n",
            " ___________\n",
            "0.21611157059669495\n",
            "[0.5675860654996292]\n",
            "\n",
            " ___________\n",
            "0.21578457951545715\n",
            "[0.5630996243208529]\n",
            "\n",
            " ___________\n",
            "0.21548143029212952\n",
            "[0.5683730017547773]\n",
            "\n",
            " ___________\n",
            "0.21519960463047028\n",
            "[0.5670795318181543]\n",
            "\n",
            " ___________\n",
            "0.2149372398853302\n",
            "[0.5707579311717209]\n",
            "\n",
            " ___________\n",
            "0.21469396352767944\n",
            "[0.572621251500003]\n",
            "\n",
            " ___________\n",
            "0.21446795761585236\n",
            "[0.5742705010462333]\n",
            "\n",
            " ___________\n",
            "0.21425823867321014\n",
            "[0.5646131952024024]\n",
            "\n",
            " ___________\n",
            "0.2140638381242752\n",
            "[0.572310698111956]\n",
            "\n",
            " ___________\n",
            "0.21388272941112518\n",
            "[0.5819830793629737]\n",
            "\n",
            " ___________\n",
            "0.2137155830860138\n",
            "[0.5873680148100799]\n",
            "\n",
            " ___________\n",
            "0.21356050670146942\n",
            "[0.5915951589852441]\n",
            "\n",
            " ___________\n",
            "0.21341648697853088\n",
            "[0.6021992004004028]\n",
            "\n",
            "\n",
            "___________\n",
            "0.213282972574234\n",
            "[0.6050786031730717]\n",
            "\n",
            " ___________\n",
            "0.21315909922122955\n",
            "[0.6096132856548455]\n",
            "\n",
            " ___________\n",
            "0.2130444347858429\n",
            "[0.6089228320056924]\n",
            "\n",
            " ___________\n",
            "0.21293774247169495\n",
            "[0.6198736077861463]\n",
            "\n",
            " ___________\n",
            "0.21283945441246033\n",
            "[0.6261992486417057]\n",
            "\n",
            " ___________\n",
            "0.2127479612827301\n",
            "[0.5926896335470021]\n",
            "\n",
            " ___________\n",
            "0.2126636803150177\n",
            "[0.568050388040981]\n",
            "\n",
            " ___________\n",
            "0.21258538961410522\n",
            "[0.567429281264887]\n",
            "\n",
            " ___________\n",
            "0.21251247823238373\n",
            "[0.5803157393281192]\n",
            "\n",
            " ___________\n",
            "0.2124451994895935\n",
            "[0.5622674618441443]\n",
            "\n",
            " ___________\n",
            "0.21238301694393158\n",
            "[0.565541840285106]\n",
            "\n",
            " ___________\n",
            "0.2123258113861084\n",
            "[0.5788594549938794]\n",
            "\n",
            " ___________\n",
            "0.21227234601974487\n",
            "[0.5799147334969518]\n",
            "\n",
            " ___________\n",
            "0.21222364902496338\n",
            "[0.5605066542847322]\n",
            "\n",
            " ___________\n",
            "0.2121775895357132\n",
            "[0.563678519956824]\n",
            "\n",
            " ___________\n",
            "0.21213527023792267\n",
            "[0.5420272201552164]\n",
            "\n",
            " ___________\n",
            "0.21209660172462463\n",
            "[0.5450965730584383]\n",
            "\n",
            " ___________\n",
            "0.21206040680408478\n",
            "[0.552441311439822]\n",
            "\n",
            " ___________\n",
            "0.21202726662158966\n",
            "[0.5241085911730475]\n",
            "\n",
            " ___________\n",
            "0.21199604868888855\n",
            "[0.5272804568451394]\n",
            "\n",
            "\n",
            "___________\n",
            "0.21196730434894562\n",
            "[0.5565991087419271]\n",
            "\n",
            " ___________\n",
            "0.21194066107273102\n",
            "[0.5617307773482961]\n",
            "\n",
            " ___________\n",
            "0.2119160294532776\n",
            "[0.5588031332726296]\n",
            "\n",
            " ___________\n",
            "0.21189364790916443\n",
            "[0.5722684869718331]\n",
            "\n",
            " ___________\n",
            "0.21187272667884827\n",
            "[0.5581187097863514]\n",
            "\n",
            " ___________\n",
            "0.2118537724018097\n",
            "[0.5666604354983628]\n",
            "\n",
            " ___________\n",
            "0.21183617413043976\n",
            "[0.5791639782190516]\n",
            "\n",
            " ___________\n",
            "0.21181908249855042\n",
            "[0.5718735113035402]\n",
            "\n",
            " ___________\n",
            "0.2118035852909088\n",
            "[0.5868644962100427]\n",
            "\n",
            " ___________\n",
            "0.21178920567035675\n",
            "[0.5945197879794734]\n",
            "\n",
            " ___________\n",
            "0.21177613735198975\n",
            "[0.606685641579179]\n",
            "\n",
            " ___________\n",
            "0.21176376938819885\n",
            "[0.6169339033847304]\n",
            "\n",
            " ___________\n",
            "0.21175293624401093\n",
            "[0.6113439423998841]\n",
            "\n",
            " ___________\n",
            "0.21174287796020508\n",
            "[0.5927258145242502]\n",
            "\n",
            " ___________\n",
            "0.2117328941822052\n",
            "[0.6047198084820271]\n",
            "\n",
            " ___________\n",
            "0.2117236703634262\n",
            "[0.6047680497850246]\n",
            "\n",
            " ___________\n",
            "0.2117154747247696\n",
            "[0.5884021877430909]\n",
            "\n",
            " ___________\n",
            "0.21170830726623535\n",
            "[0.5885619870592704]\n",
            "\n",
            " ___________\n",
            "0.21170105040073395\n",
            "[0.5881368605766042]\n",
            "\n",
            " ___________\n",
            "0.21169450879096985\n",
            "[0.5960966755712072]\n",
            "\n",
            "\n",
            "___________\n",
            "0.21168826520442963\n",
            "[0.5890172643563103]\n",
            "\n",
            " ___________\n",
            "0.21168221533298492\n",
            "[0.5774785476955733]\n",
            "\n",
            " ___________\n",
            "0.21167755126953125\n",
            "[0.5591860486151731]\n",
            "\n",
            " ___________\n",
            "0.21167241036891937\n",
            "[0.5394945517478427]\n",
            "\n",
            " ___________\n",
            "0.2116680145263672\n",
            "[0.5444453154679708]\n",
            "\n",
            " ___________\n",
            "0.21166355907917023\n",
            "[0.545232251723119]\n",
            "\n",
            " ___________\n",
            "0.21165931224822998\n",
            "[0.5573076528797043]\n",
            "\n",
            " ___________\n",
            "0.2116561233997345\n",
            "[0.5784765396513359]\n",
            "\n",
            " ___________\n",
            "0.2116529643535614\n",
            "[0.6023801052866437]\n",
            "\n",
            " ___________\n",
            "0.21164968609809875\n",
            "[0.609194189335054]\n",
            "\n",
            " ___________\n",
            "0.21164722740650177\n",
            "[0.6317168476720556]\n",
            "\n",
            " ___________\n",
            "0.21164438128471375\n",
            "[0.6197228537142788]\n",
            "\n",
            " ___________\n",
            "0.2116415649652481\n",
            "[0.6257409562632286]\n",
            "\n",
            " ___________\n",
            "0.21163946390151978\n",
            "[0.6255751267841745]\n",
            "\n",
            " ___________\n",
            "0.21163731813430786\n",
            "[0.6225298945324513]\n",
            "\n",
            " ___________\n",
            "0.21163566410541534\n",
            "[0.6368786670927981]\n",
            "\n",
            " ___________\n",
            "0.21163423359394073\n",
            "[0.639938974751708]\n",
            "\n",
            " ___________\n",
            "0.21163253486156464\n",
            "[0.6404756592475563]\n",
            "\n",
            " ___________\n",
            "0.2116309553384781\n",
            "[0.6245922102355984]\n",
            "\n",
            " ___________\n",
            "0.21162934601306915\n",
            "[0.6305922222959242]\n",
            "\n",
            "\n",
            "___________\n",
            "0.21162791550159454\n",
            "[0.6268263855806746]\n",
            "\n",
            " ___________\n",
            "0.2116270512342453\n",
            "[0.635178161162133]\n",
            "\n",
            " ___________\n",
            "0.21162571012973785\n",
            "[0.6475339648923917]\n",
            "\n",
            " ___________\n",
            "0.21162500977516174\n",
            "[0.6484535647307834]\n",
            "\n",
            " ___________\n",
            "0.21162408590316772\n",
            "[0.6471359741426616]\n",
            "\n",
            " ___________\n",
            "0.21162298321723938\n",
            "[0.6506606043429233]\n",
            "\n",
            " ___________\n",
            "0.21162232756614685\n",
            "[0.6530455337598668]\n",
            "\n",
            " ___________\n",
            "0.21162155270576477\n",
            "[0.6574174018440239]\n",
            "\n",
            " ___________\n",
            "0.2116207480430603\n",
            "[0.6582194135063588]\n",
            "\n",
            " ___________\n",
            "0.21162015199661255\n",
            "[0.6653922922458135]\n",
            "\n",
            " ___________\n",
            "0.21161949634552002\n",
            "[0.6618706771269892]\n",
            "\n",
            " ___________\n",
            "0.21161897480487823\n",
            "[0.6687842588628319]\n",
            "\n",
            " ___________\n",
            "0.21161822974681854\n",
            "[0.6546586023288489]\n",
            "\n",
            " ___________\n",
            "0.21161790192127228\n",
            "[0.6537390024904572]\n",
            "\n",
            " ___________\n",
            "0.21161767840385437\n",
            "[0.6595249437687312]\n",
            "\n",
            " ___________\n",
            "0.2116173803806305\n",
            "[0.6568535816152393]\n",
            "\n",
            " ___________\n",
            "0.21161730587482452\n",
            "[0.6658505846242907]\n",
            "\n",
            " ___________\n",
            "0.21161693334579468\n",
            "[0.671096826325279]\n",
            "\n",
            " ___________\n",
            "0.21161694824695587\n",
            "[0.6737320075015225]\n",
            "\n",
            " ___________\n",
            "0.21161670982837677\n",
            "[0.6622415321437832]\n",
            "\n",
            "\n",
            "___________\n",
            "0.2116168737411499\n",
            "[0.6664023445273256]\n",
            "\n",
            " ___________\n",
            "0.21161691844463348\n",
            "[0.6649973165775207]\n",
            "\n",
            " ___________\n",
            "0.21161681413650513\n",
            "[0.6651812365451991]\n",
            "\n",
            " ___________\n",
            "0.21161691844463348\n",
            "[0.673924972713513]\n",
            "\n",
            " ___________\n",
            "0.21161681413650513\n",
            "[0.6820596624314823]\n",
            "\n",
            " ___________\n",
            "0.21161691844463348\n",
            "[0.6778928198850651]\n",
            "\n",
            " ___________\n",
            "0.21161681413650513\n",
            "[0.677012416105359]\n",
            "\n",
            " ___________\n",
            "0.21161691844463348\n",
            "[0.6864616813300127]\n",
            "\n",
            " ___________\n",
            "0.21161681413650513\n",
            "[0.6663842540387015]\n",
            "\n",
            " ___________\n",
            "0.21161691844463348\n",
            "[0.6706596395168632]\n",
            "\n",
            " ___________\n",
            "0.21161681413650513\n",
            "[0.6864918321443862]\n",
            "\n",
            " ___________\n",
            "0.21161691844463348\n",
            "[0.7016697521000043]\n",
            "\n",
            " ___________\n",
            "0.21161681413650513\n",
            "[0.6980787901081208]\n",
            "\n",
            " ___________\n",
            "0.21161691844463348\n",
            "[0.6976204977296436]\n",
            "\n",
            " ___________\n",
            "0.21161681413650513\n",
            "[0.6978044176973219]\n",
            "\n",
            " ___________\n",
            "0.21161691844463348\n",
            "[0.7016486465299427]\n",
            "\n",
            " ___________\n",
            "0.21161681413650513\n",
            "[0.6937340577569]\n",
            "\n",
            " ___________\n",
            "0.21161691844463348\n",
            "[0.6916687269723155]\n",
            "\n",
            " ___________\n",
            "0.21161681413650513\n",
            "[0.702022516628174]\n",
            "\n",
            " ___________\n",
            "0.21161691844463348\n",
            "[0.7144567124758039]\n",
            "\n",
            "\n",
            "___________\n",
            "0.21161681413650513\n",
            "[0.7104707748156277]\n",
            "\n",
            " ___________\n",
            "0.21161691844463348\n",
            "[0.7127139954050158]\n",
            "\n",
            " ___________\n",
            "0.21161681413650513\n",
            "[0.7161150072663462]\n",
            "\n",
            " ___________\n",
            "0.21161691844463348\n",
            "[0.7082365994705516]\n",
            "\n",
            " ___________\n",
            "0.21161681413650513\n",
            "[0.6837903191765209]\n",
            "\n",
            " ___________\n",
            "0.21161691844463348\n",
            "[0.6905139507818107]\n",
            "\n",
            " ___________\n",
            "0.21161681413650513\n",
            "[0.6763792490035155]\n",
            "\n",
            " ___________\n",
            "0.21161691844463348\n",
            "[0.6946717480839157]\n",
            "\n",
            " ___________\n",
            "0.21161681413650513\n",
            "[0.7092737874849999]\n",
            "\n",
            " ___________\n",
            "0.21161691844463348\n",
            "[0.7081853430861167]\n",
            "\n",
            " ___________\n",
            "0.21161681413650513\n",
            "[0.7207401421912406]\n",
            "\n",
            " ___________\n",
            "0.21161691844463348\n",
            "[0.7250486935652132]\n",
            "\n",
            " ___________\n",
            "0.21161681413650513\n",
            "[0.7304456893380689]\n",
            "\n",
            " ___________\n",
            "0.21161691844463348\n",
            "[0.7335482081371018]\n",
            "\n",
            " ___________\n",
            "0.21161681413650513\n",
            "[0.7322276024675426]\n",
            "\n",
            " ___________\n",
            "0.21161691844463348\n",
            "[0.7307954387848016]\n",
            "\n",
            " ___________\n",
            "0.21161681413650513\n",
            "[0.7392256064836311]\n",
            "\n",
            " ___________\n",
            "0.21161691844463348\n",
            "[0.7328607695693861]\n",
            "\n",
            " ___________\n",
            "0.21161681413650513\n",
            "[0.7320647880699258]\n",
            "\n",
            " \n",
            " \n",
            "!!!!!!! Text generation Ends !!!!!!!\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Kmrj7XdhnZ-8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}