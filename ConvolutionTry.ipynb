{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/plantehenry/NeuralNetworksFinalProject/blob/main/ConvolutionTry.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.datasets import CIFAR100, CIFAR10\n",
        "from torch.utils.data import DataLoader, Dataset, random_split\n",
        "from torchvision.transforms.functional import resize\n",
        "from torchvision.transforms import CenterCrop\n",
        "from torchvision.transforms import ToTensor\n",
        "from torchvision.io import read_image\n",
        "from torchsummary import summary\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "nfCs7DH7ex1L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "PgNzdFYEpg_S",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "167e6c94-80e0-4f47-bf0d-3a7945e9a500"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define Network"
      ],
      "metadata": {
        "id": "pSjILg0Se0_M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CNN(nn.Module):\n",
        "\n",
        "    def __init__(self, numChannels):\n",
        "        super(CNN, self).__init__()\n",
        "        # try conv 1d \n",
        "        #back to 5 layers\n",
        "        # Convolutional layers:\n",
        "        self.conv1 = nn.Conv1d(in_channels = numChannels, out_channels = 25, kernel_size = 7, stride = 1)\n",
        "        self.conv2 = nn.Conv1d(in_channels = 25, out_channels = 50, kernel_size = 5, stride = 1)\n",
        "        self.conv3 = nn.Conv1d(in_channels = 50, out_channels = 75, kernel_size = 3, stride = 1)\n",
        "        self.conv4 = nn.Conv1d(in_channels = 75, out_channels = 100, kernel_size = 3, stride = 1)\n",
        "        self.conv5 = nn.Conv1d(in_channels = 100, out_channels = 150, kernel_size = 3, stride = 1)\n",
        "\n",
        "        self.relu = nn.ReLU()\n",
        "        self.tanh = nn.Tanh()\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size = (1,2), stride = (1,2))\n",
        "\n",
        "        # Batch normalization layers:\n",
        "        self.batchnorm1 = nn.BatchNorm2d(num_features = 30)\n",
        "        self.batchnorm2 = nn.BatchNorm2d(num_features = 50)\n",
        "\n",
        "        #BACK TO 2 OR 3 FULLY CONNECTED LAYER\n",
        "        # Fully-connected layers:\n",
        "        #ue sigmoid for all fully connecetd layers \n",
        "        # or tanh in first two and last one sigmoid\n",
        "        self.fc1 = nn.Linear(in_features = 450, out_features= 50)\n",
        "        self.fc2 = nn.Linear(in_features = 50, out_features= 25)\n",
        "        self.fc3 = nn.Linear(in_features = 25, out_features= 2)\n",
        "\n",
        "        #no dropout ot batchnorm initially\n",
        "        # try with droput last\n",
        "        self.dropout1 = nn.Dropout2d(p = args.dropoutRate)\n",
        "        self.dropout2 = nn.Dropout2d(p = args.dropoutRate)\n",
        "        self.dropout3 = nn.Dropout2d(p = args.dropoutRate)\n",
        "        self.dropout4 = nn.Dropout(p = args.dropoutRate)\n",
        "\n",
        "    # Evaluation function\n",
        "    def evaluate(self, model, dataloader, device):\n",
        "        model.eval()\n",
        "        running_loss = 0.0\n",
        "        criterion = nn.MSELoss()  \n",
        "        for data in dataloader:\n",
        "                inputs, true = data\n",
        "                inputs = inputs.to(device)\n",
        "                true = true.to(device)\n",
        "                outputs = model(inputs)\n",
        "\n",
        "                loss = criterion(outputs, true)\n",
        "\n",
        "                loss = loss.detach().cpu().numpy()\n",
        "                running_loss += loss\n",
        "        return(running_loss/ dataloader.__len__())\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.relu(x)\n",
        "        # x = self.dropout1(x)\n",
        "        x = self.maxpool(x)\n",
        "        # x = self.batchnorm1(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.relu(x)\n",
        "        # x = self.dropout2(x)\n",
        "        x = self.maxpool(x)\n",
        "        # x = self.batchnorm2(x)\n",
        "        x = self.conv3(x)\n",
        "        x = self.relu(x)\n",
        "        # x = self.dropout3(x)\n",
        "        x = self.conv4(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.conv5(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.maxpool(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "\n",
        "        \n",
        "        x = self.fc1(x)\n",
        "        x = self.tanh(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.tanh(x)\n",
        "        x = self.fc3(x)\n",
        "        # x = self.sigmoid(x)\n",
        "        # outmap_min, _ = torch.min(x, dim=1, keepdim=True)\n",
        "        # outmap_max, _ = torch.max(x, dim=1, keepdim=True)\n",
        "        # x = (x - outmap_min) / (outmap_max - outmap_min)\n",
        "\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "bU3lPzXCXOpj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Loaders"
      ],
      "metadata": {
        "id": "RyhmhjrG4Qt9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import argparse\n",
        "from torch.utils.data.dataloader import default_collate\n",
        "import csv\n",
        "\n",
        "\n",
        "class TrainData(torch.utils.data.Dataset):\n",
        "    def __init__(self,args):\n",
        "        self.args = args\n",
        "        self.input_sequence , self.output_sequence = self.loadData()\n",
        "    ## working on giving more than one sequence\n",
        "    def loadData(self):\n",
        "        # Read the text\n",
        "        input_sequence = []\n",
        "        with open(f\"{self.args.workingDir}/{self.args.inputFile}.csv\", newline='') as csvfile:\n",
        "          spamreader = csv.reader(csvfile, delimiter=',', quotechar='|')\n",
        "          spamreader.__next__()\n",
        "          for row in spamreader:\n",
        "              new_row = []\n",
        "              \n",
        "              # wrap each value in row in an array\n",
        "              for val in row:\n",
        "                new_row.append([float(val)])\n",
        "\n",
        "              # if this is the first row will need to initialize input sequence with arrays\n",
        "              while len(new_row) > len(input_sequence):\n",
        "                input_sequence.append([])\n",
        "              \n",
        "              # put each wrapped value in the correct index \n",
        "              for idx in range(len(new_row)):\n",
        "                input_sequence[idx].append(new_row[idx])\n",
        "        test_len = int(args.test_split * len(input_sequence[0]))\n",
        "        input_sequence = torch.tensor(input_sequence)[:, 0:test_len, :]\n",
        "\n",
        "\n",
        "\n",
        "        output_sequence = []\n",
        "        with open(f\"{self.args.workingDir}/{self.args.outputFile}.csv\", newline='') as csvfile:\n",
        "          spamreader = csv.reader(csvfile, delimiter=',', quotechar='|')\n",
        "          spamreader.__next__()\n",
        "          for row in spamreader:\n",
        "            new_row = []\n",
        "            for val in row:\n",
        "              new_row.append(float(val) * 100)\n",
        "            output_sequence.append(new_row)\n",
        "        output_sequence = torch.tensor(output_sequence)[0:test_len]\n",
        "        return input_sequence, output_sequence\n",
        "\n",
        "    def __len__(self):\n",
        "        # Get the number of sequences for training purpose.\n",
        "        return len(self.input_sequence[0]) - self.args.seqLength\n",
        "\n",
        "    # returns tensor is size [num_features, 1, seq_length]\n",
        "    def __getitem__(self, index):\n",
        "        return_seq = self.input_sequence[:, index:index+self.args.seqLength, :].transpose(1, 2).detach().clone()\n",
        "        return_seq = torch.squeeze(return_seq)\n",
        "        # need to apply normalization to input \n",
        "        # normalization squeezes between 0 and 1 while keeping percent change between values the same\n",
        "        # each feature sequence that I want to noralize gets divided by its max value\n",
        "\n",
        "        # stock, bond, and real estate price\n",
        "        # normalize_idxs = [0, 1]\n",
        "\n",
        "        # # get mask with size of features with 1s where you want to normlaize\n",
        "        # mask = torch.zeros(return_seq.shape[0], dtype=torch.bool)\n",
        "        # mask[normalize_idxs] = True\n",
        "\n",
        "        # # get max values of indees you want to normaize\n",
        "        # max_vals, _ = return_seq[mask].max(dim = 2, keepdim =True)\n",
        "\n",
        "        # # for indexes you want to nomalize divide by max value\n",
        "        # return_seq[mask] /= max_vals\n",
        "        return (\n",
        "            return_seq,\n",
        "            self.output_sequence[index+self.args.seqLength - 1],\n",
        "        )\n",
        "\n",
        "\n",
        "class ValData(torch.utils.data.Dataset):\n",
        "    def __init__(self,args):\n",
        "        self.args = args\n",
        "        self.input_sequence , self.output_sequence = self.loadData()\n",
        "\n",
        "    def loadData(self):\n",
        "        # Read the text\n",
        "        input_sequence = []\n",
        "        with open(f\"{self.args.workingDir}/{self.args.inputFile}.csv\", newline='') as csvfile:\n",
        "          spamreader = csv.reader(csvfile, delimiter=',', quotechar='|')\n",
        "          spamreader.__next__()\n",
        "          for row in spamreader:\n",
        "              new_row = []\n",
        "              for val in row:\n",
        "                new_row.append([float(val)])\n",
        "              while len(new_row) > len(input_sequence):\n",
        "                input_sequence.append([])\n",
        "              for idx in range(len(new_row)):\n",
        "                input_sequence[idx].append(new_row[idx])\n",
        "        test_len = int(args.test_split * len(input_sequence[0]))\n",
        "        input_sequence = torch.tensor(input_sequence)[:, test_len: , :]\n",
        "\n",
        "        output_sequence = []\n",
        "        with open(f\"{self.args.workingDir}/{self.args.outputFile}.csv\", newline='') as csvfile:\n",
        "          spamreader = csv.reader(csvfile, delimiter=',', quotechar='|')\n",
        "          spamreader.__next__()\n",
        "          for row in spamreader:\n",
        "            new_row = []\n",
        "            for val in row:\n",
        "              new_row.append(float(val) * 100)\n",
        "            output_sequence.append(new_row)\n",
        "        output_sequence = torch.tensor(output_sequence)[test_len: ]\n",
        "        return input_sequence, output_sequence\n",
        "\n",
        "    def __len__(self):\n",
        "        # Get the number of sequences for training purpose.\n",
        "        return len(self.input_sequence[0]) - self.args.seqLength\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        return_seq = self.input_sequence[:, index:index+self.args.seqLength, :].transpose(1, 2).detach().clone()\n",
        "        return_seq = torch.squeeze(return_seq)\n",
        "        # need to apply normalization to input \n",
        "        # normalization squeezes between 0 and 1 while keeping percent change between values the same\n",
        "        # each feature sequence that I want to noralize gets divided by its max value\n",
        "\n",
        "        # # stock, bond, and real estate price\n",
        "        # normalize_idxs = [0, 1]\n",
        "\n",
        "        # # get mask with size of features with 1s where you want to normlaize\n",
        "        # mask = torch.zeros(return_seq.shape[0], dtype=torch.bool)\n",
        "        # mask[normalize_idxs] = True\n",
        "\n",
        "        # # get max values of indees you want to normaize\n",
        "        # max_vals, _ = return_seq[mask].max(dim = 2, keepdim =True)\n",
        "\n",
        "        # # for indexes you want to nomalize divide by max value\n",
        "        # return_seq[mask] /= max_vals\n",
        "\n",
        "        return (\n",
        "            return_seq,\n",
        "            self.output_sequence[index+self.args.seqLength -1],\n",
        "        )"
      ],
      "metadata": {
        "id": "9WXROY0Z4Sss"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument('-f')\n",
        "parser.add_argument('--workingDir', type=str, \n",
        "    default=\"/content/drive/My Drive/Neural Networks Project\")\n",
        "# Can be game_of_thrones or wonder_land:\n",
        "parser.add_argument('--inputFile', type=str, default=\"input_data\") \n",
        "parser.add_argument('--outputFile', type=str, default=\"output_data\") \n",
        "parser.add_argument('--maxEpochs', type=int, default=50)\n",
        "parser.add_argument('--batchSize', type=int, default=12)\n",
        "parser.add_argument('--seqLength', type=int, default=63)\n",
        "parser.add_argument('--learningRate', type=float, default=.00001)\n",
        "parser.add_argument('--dropoutRate', type=float, default=0.0)\n",
        "parser.add_argument('--test_split', type=float, default=.9)\n",
        "parser.add_argument('--num_channels', type=float, default=2)\n",
        "args = parser.parse_args()"
      ],
      "metadata": {
        "id": "5GUSyYRanh58"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = TrainData(args)\n",
        "train_loader = DataLoader(dataset,  batch_size=args.batchSize, shuffle=False, drop_last=False, num_workers=2)\n",
        "valset = ValData(args)\n",
        "val_loader = DataLoader(valset, batch_size=args.batchSize, shuffle=False, drop_last=False, num_workers=2)\n",
        "data_iter = iter(train_loader)\n",
        "print(dataset.__len__())\n",
        "print(valset.__len__())"
      ],
      "metadata": {
        "id": "ox0ZhjS7nVO8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d2a0e14d-0947-4f6e-8432-6457bf3fab34"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4222\n",
            "414\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### The main training and evaluation code starts here"
      ],
      "metadata": {
        "id": "35KXsx9VGiWx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':\n",
        "    # Specify the operation mode:\n",
        "    # 'train' = training with your train and validation data splits\n",
        "    # 'eval'  = evaluation of the trained model with your test data split \n",
        "    mode = 'train'\n",
        "\n",
        "    # Path where you plan to save the best model during training\n",
        "    my_best_model = \"/content/drive/MyDrive/Convolution_best_model.pth\"\n",
        "\n",
        "    # Set the device (GPU or CPU, depending on availability)\n",
        "    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "    print(\"Currently using device: \", device)\n",
        "\n",
        "    # Initialize the model and print out its configuration\n",
        "    model = CNN(numChannels = args.num_channels)\n",
        "    model.to(device)\n",
        "    print(\"\\n\\nModel summary:\\n\\n\")\n",
        "    summary(model, input_size=(args.num_channels, 63))\n",
        "\n",
        "    if mode == \"train\":\n",
        "\n",
        "        print(\"\\n\\nTraining starts!\\n\\n\")\n",
        "        \n",
        "        model.train()\n",
        "        criterion = nn.MSELoss()\n",
        "        optimizer = torch.optim.Adam(model.parameters(), lr=args.learningRate)\n",
        "        # optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.5)\n",
        "        \n",
        "        best_val_loss = float('inf')\n",
        "        for epoch in range(args.maxEpochs):\n",
        "            running_loss = .0\n",
        "            print(f\"Starting epoch {epoch + 1}\")\n",
        "            bestLoss = float('inf')\n",
        "            for idx, data in tqdm(enumerate(train_loader), total=len(train_loader)):\n",
        "                # Get the inputs (data is a list of [inputs, labels])\n",
        "              \n",
        "                inputs, labels = data\n",
        "                inputs = inputs.to(device)\n",
        "                labels = labels.to(device)\n",
        "                optimizer.zero_grad()\n",
        "                outputs = model(inputs)\n",
        "                \n",
        "                loss = criterion(outputs, labels)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "                loss = loss.detach().cpu().numpy()\n",
        "                inputs = inputs.detach().cpu().numpy()\n",
        "                labels = labels.detach().cpu().numpy()\n",
        "                running_loss += loss\n",
        "              \n",
        "            \n",
        "                currLoss = loss.item()\n",
        "                \n",
        "                # Only save models with smallest loss per epoch.\n",
        "                if currLoss < bestLoss:\n",
        "                    bestLoss = currLoss\n",
        "                    # torch.save(model.state_dict(), my_best_model)\n",
        "            print(f\"Epoch ID: {epoch}, 'the ave loss': {running_loss/ train_loader.__len__()}\")\n",
        "\n",
        "            # Evaluate the accuracy after each epoch\n",
        "            val_loss = model.evaluate(model, val_loader, device)\n",
        "            print(f\"Epoch ID: {epoch}, 'the ave val loss': {val_loss}\")\n",
        "            if val_loss < best_val_loss:\n",
        "                print(f\"Better validation accuracy achieved: {val_loss}\")\n",
        "                best_val_loss = val_loss\n",
        "                print(f\"Saving this model as: {my_best_model}\")\n",
        "                torch.save(model.state_dict(), my_best_model)\n",
        "\n",
        "    # And here we evaluate the trained model with the test data\n",
        "    # elif mode == \"eval\":\n",
        "\n",
        "    #     print(\"\\n\\nValidating the trained model:\")\n",
        "    #     print(f\"Loading checkpoint from {my_best_model}\")\n",
        "    #     model.load_state_dict(torch.load(my_best_model))\n",
        "    #     acc = model.evaluate(model, test_loader, classes, device)\n",
        "    #     print(f\"Accuracy on the test (unknown) data: {acc * 100:.2f}%\")\n",
        "\n",
        "    else:\n",
        "        print(\"'mode' argument should either be 'train' or 'eval'\")"
      ],
      "metadata": {
        "id": "yy5S2JkWgzsj",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "1f35635c-d9be-4557-c484-64d303519b41"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Currently using device:  cuda:0\n",
            "\n",
            "\n",
            "Model summary:\n",
            "\n",
            "\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv1d-1               [-1, 25, 57]             375\n",
            "              ReLU-2               [-1, 25, 57]               0\n",
            "         MaxPool2d-3               [-1, 25, 28]               0\n",
            "            Conv1d-4               [-1, 50, 24]           6,300\n",
            "              ReLU-5               [-1, 50, 24]               0\n",
            "         MaxPool2d-6               [-1, 50, 12]               0\n",
            "            Conv1d-7               [-1, 75, 10]          11,325\n",
            "              ReLU-8               [-1, 75, 10]               0\n",
            "            Conv1d-9               [-1, 100, 8]          22,600\n",
            "             ReLU-10               [-1, 100, 8]               0\n",
            "           Conv1d-11               [-1, 150, 6]          45,150\n",
            "             ReLU-12               [-1, 150, 6]               0\n",
            "        MaxPool2d-13               [-1, 150, 3]               0\n",
            "           Linear-14                   [-1, 50]          22,550\n",
            "             Tanh-15                   [-1, 50]               0\n",
            "           Linear-16                   [-1, 25]           1,275\n",
            "             Tanh-17                   [-1, 25]               0\n",
            "           Linear-18                    [-1, 2]              52\n",
            "================================================================\n",
            "Total params: 109,627\n",
            "Trainable params: 109,627\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.00\n",
            "Forward/backward pass size (MB): 0.09\n",
            "Params size (MB): 0.42\n",
            "Estimated Total Size (MB): 0.51\n",
            "----------------------------------------------------------------\n",
            "\n",
            "\n",
            "Training starts!\n",
            "\n",
            "\n",
            "Starting epoch 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 352/352 [00:03<00:00, 103.77it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch ID: 0, 'the ave loss': 11.428023681552572\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch ID: 0, 'the ave val loss': 13.812179608004433\n",
            "Better validation accuracy achieved: 13.812179608004433\n",
            "Saving this model as: /content/drive/MyDrive/Convolution_best_model.pth\n",
            "Starting epoch 2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 352/352 [00:03<00:00, 103.40it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch ID: 1, 'the ave loss': 11.305033426424911\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch ID: 1, 'the ave val loss': 14.015354556696755\n",
            "Starting epoch 3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 352/352 [00:02<00:00, 138.73it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch ID: 2, 'the ave loss': 11.260909716641022\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch ID: 2, 'the ave val loss': 14.019117442199162\n",
            "Starting epoch 4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 352/352 [00:02<00:00, 140.25it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch ID: 3, 'the ave loss': 11.25267075758893\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch ID: 3, 'the ave val loss': 14.023420510121754\n",
            "Starting epoch 5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 352/352 [00:02<00:00, 136.34it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch ID: 4, 'the ave loss': 11.25004433424593\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch ID: 4, 'the ave val loss': 14.027836883493832\n",
            "Starting epoch 6\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 352/352 [00:03<00:00, 115.65it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch ID: 5, 'the ave loss': 11.248088899047367\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch ID: 5, 'the ave val loss': 14.033480204003197\n",
            "Starting epoch 7\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 352/352 [00:03<00:00, 116.70it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch ID: 6, 'the ave loss': 11.247343970113434\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch ID: 6, 'the ave val loss': 14.036391083683286\n",
            "Starting epoch 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 352/352 [00:02<00:00, 135.82it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch ID: 7, 'the ave loss': 11.245469773632728\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch ID: 7, 'the ave val loss': 14.040429468240056\n",
            "Starting epoch 9\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 352/352 [00:02<00:00, 138.48it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch ID: 8, 'the ave loss': 11.245133441269651\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch ID: 8, 'the ave val loss': 14.043879890441895\n",
            "Starting epoch 10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 352/352 [00:02<00:00, 129.73it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch ID: 9, 'the ave loss': 11.24374291641553\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch ID: 9, 'the ave val loss': 14.04756463766098\n",
            "Starting epoch 11\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 352/352 [00:03<00:00, 107.11it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch ID: 10, 'the ave loss': 11.236430358691988\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch ID: 10, 'the ave val loss': 14.125831777708871\n",
            "Starting epoch 12\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 352/352 [00:02<00:00, 141.33it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch ID: 11, 'the ave loss': 11.243452804649927\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch ID: 11, 'the ave val loss': 14.119814716918128\n",
            "Starting epoch 13\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 48%|████▊     | 169/352 [00:01<00:01, 138.98it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-844c0163057e>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m                 \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m                 \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    485\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m             )\n\u001b[0;32m--> 487\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    488\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    198\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    201\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "checkpointFile = \"/content/drive/MyDrive/Convolution_best_model\"\n",
        "\n",
        "# If using GPU, we need the following line.\n",
        "model.to(device)\n",
        "\n",
        "model.load_state_dict(torch.load(f\"{checkpointFile}.pth\"))\n",
        "\n",
        "\n",
        "generatingLoader = DataLoader(valset, batch_size=1, shuffle=True)\n",
        "\n",
        "\n",
        "# Get one data point from the DataLoader\n",
        "data_point, true = next(iter(generatingLoader))\n",
        "data_point = data_point.to(device)\n",
        "print(data_point)\n",
        "print(true)\n",
        "# model.eval()\n",
        "torch.no_grad()\n",
        "print(model(data_point))"
      ],
      "metadata": {
        "id": "CBuoY8J3dWwJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b7a06795-92de-479d-c964-052d21a9a5f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[4701.4800, 4707.2500, 4670.2598, 4659.3901, 4655.2402, 4689.2998,\n",
            "          4679.4199, 4701.5000, 4700.7202, 4708.4399, 4712.0000, 4678.4800,\n",
            "          4675.7798, 4664.6299, 4628.7500, 4640.2500, 4602.8198, 4504.7300,\n",
            "          4589.4902, 4548.3701, 4631.9702, 4690.8599, 4691.0000, 4687.6401,\n",
            "          4710.2998, 4642.9902, 4636.4600, 4719.1299, 4652.5000, 4587.8999,\n",
            "          4594.9600, 4650.3599, 4703.9600, 4733.9902, 4795.4902, 4788.6401,\n",
            "          4794.2300, 4775.2100, 4778.1401, 4804.5098, 4787.9902, 4693.3901,\n",
            "          4697.6602, 4655.3398, 4669.1401, 4728.5898, 4733.5601, 4637.9902,\n",
            "          4632.2402, 4588.0298, 4547.3501, 4471.3799, 4356.3198, 4366.6401,\n",
            "          4408.4302, 4380.5801, 4336.1899, 4431.7900, 4519.5698, 4566.3901,\n",
            "          4535.4102, 4482.7900, 4505.7500],\n",
            "         [ 115.1400,  115.3800,  115.1100,  114.5100,  114.5000,  114.3000,\n",
            "           113.9200,  113.7900,  114.0500,  114.4900,  114.1400,  113.7100,\n",
            "           113.4700,  114.3300,  114.1500,  114.8700,  114.2700,  114.6700,\n",
            "           114.5000,  114.9800,  114.5200,  114.3700,  114.2800,  114.3700,\n",
            "           114.5100,  114.4600,  114.2300,  114.2600,  114.5500,  114.5700,\n",
            "           114.0700,  114.3300,  114.2900,  114.1500,  114.4300,  113.9900,\n",
            "           113.9900,  114.1500,  113.6700,  113.2200,  113.3400,  112.7500,\n",
            "           112.6900,  112.2300,  112.3300,  112.7200,  112.6000,  112.5300,\n",
            "           111.8100,  111.7000,  111.8400,  112.1900,  112.3800,  112.1800,\n",
            "           112.0600,  111.6200,  111.4600,  111.6400,  111.6600,  111.7000,\n",
            "           111.2600,  110.7500,  110.5700]]], device='cuda:0')\n",
            "tensor([[-6.2731, -1.3295]])\n",
            "tensor([[ 0.1298, -0.0218]], device='cuda:0', grad_fn=<AddmmBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "YzYhq6MLzKH_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}